# # simplified_image_understanding_mps.py
# import sys
# import os
# from PIL import Image
# import torch
# from transformers import AutoProcessor

# # --- æª¢æŸ¥ MPS å¯ç”¨æ€§ ---
# if not torch.backends.mps.is_available():
#     if not torch.backends.mps.is_built():
#         print("MPS not available because the current PyTorch install was not built with MPS enabled.")
#     else:
#         print("MPS not available because the current macOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.")
#     sys.exit(1)
# else:
#     print("MPS is available. Using device: mps")

# # å‡è¨­ blip3o å’Œ qwen_vl_utils åœ¨ä½ çš„ Python è·¯å¾‘æˆ–åŒä¸€ç›®éŒ„ä¸‹
# # ä½ å¯èƒ½éœ€è¦èª¿æ•´ import è·¯å¾‘
# try:
#     from blip3o.model.builder import load_pretrained_model
#     from blip3o.utils import disable_torch_init
#     from qwen_vl_utils import process_vision_info
# except ImportError as e:
#     print(f"Error importing custom modules: {e}")
#     print("Please ensure 'blip3o' and 'qwen_vl_utils' are correctly installed or in the Python path.")
#     sys.exit(1)

# # --- è¨­å®šç›®æ¨™è¨­å‚™ ---
# DEVICE = 'mps' # åœ¨ macOS ä¸Šä½¿ç”¨ MPS

# def process_image(prompt: str, img: Image.Image, model, processor) -> str:
#     """
#     ä½¿ç”¨ Qwen-VL æ¨¡å‹è™•ç†åœ–ç‰‡å’Œæç¤ºæ–‡å­—ï¼Œé€²è¡Œåœ–åƒç†è§£ã€‚
#     """
#     # 1. æº–å‚™è¨Šæ¯æ ¼å¼
#     messages = [{
#         "role": "user",
#         "content": [
#             {"type": "image", "image": img},
#             {"type": "text", "text": prompt},
#         ],
#     }]
    
#     # 2. ä½¿ç”¨ processor è™•ç†è¨Šæ¯
#     # æ³¨æ„ï¼šapply_chat_template é€šå¸¸åœ¨ CPU ä¸ŠåŸ·è¡Œï¼Œä¸éœ€è¦ç‰¹åˆ¥ç§»åˆ° MPS
#     text_prompt_for_qwen = processor.apply_chat_template(
#         messages, tokenize=False, add_generation_prompt=True
#     )
    
#     # 3. è™•ç†è¦–è¦ºè³‡è¨Š (åœ–ç‰‡/å½±ç‰‡)
#     # process_vision_info ä¹Ÿå¯èƒ½æ¶‰åŠå¼µé‡æ“ä½œï¼Œä½†é€šå¸¸è¼¸å‡ºæ˜¯ CPU list
#     # å¦‚æœå…§éƒ¨æœ‰æ¨¡å‹é‹ç®—ï¼Œå¯èƒ½éœ€è¦ä¿®æ”¹åŸå§‹ç¢¼ã€‚é€™è£¡å…ˆå‡è¨­å®ƒå…¼å®¹ã€‚
#     image_inputs, video_inputs = process_vision_info(messages)
    
#     # 4. å°‡è™•ç†å¾Œçš„è¼¸å…¥è½‰ç‚ºæ¨¡å‹æ‰€éœ€çš„å¼µé‡
#     inputs = processor(
#         text=[text_prompt_for_qwen],
#         images=image_inputs,
#         videos=video_inputs,
#         padding=True,
#         return_tensors="pt",
#     ) # å…ˆåœ¨ CPU ä¸Šå»ºç«‹å¼µé‡
    
#     # 5. ***é—œéµä¿®æ”¹*** å°‡å¼µé‡ç§»å‹•åˆ° MPS è¨­å‚™
#     inputs = inputs.to(DEVICE) 
#     print(f"Input tensors moved to device: {inputs.input_ids.device}")

#     # 6. ç”Ÿæˆå›æ‡‰ (åœ¨ MPS ä¸ŠåŸ·è¡Œ)
#     print("Generating response...")
#     generated_ids = model.generate(**inputs, max_new_tokens=1024)
#     print(f"Generated IDs device: {generated_ids.device}")
    
#     # 7. ç§»é™¤è¼¸å…¥éƒ¨åˆ†ï¼Œåªä¿ç•™ç”Ÿæˆçš„éƒ¨åˆ†
#     input_token_len = inputs.input_ids.shape[1]
#     generated_ids_trimmed = generated_ids[:, input_token_len:]
    
#     # 8. è§£ç¢¼ç”Ÿæˆçš„ token ç‚ºæ–‡å­— (é€šå¸¸åœ¨ CPU ä¸ŠåŸ·è¡Œ)
#     # å°‡çµæœç§»å› CPU é€²è¡Œè§£ç¢¼æ˜¯å¸¸è¦‹åšæ³•
#     generated_ids_trimmed_cpu = generated_ids_trimmed.to('cpu')
#     output_text = processor.batch_decode(
#         generated_ids_trimmed_cpu, skip_special_tokens=True,
#         clean_up_tokenization_spaces=False
#     )[0]
    
#     return output_text

# def main():
#     """
#     ä¸»å‡½æ•¸ï¼šè¼‰å…¥æ¨¡å‹ï¼Œç²å–è¼¸å…¥ï¼Œèª¿ç”¨è™•ç†å‡½æ•¸ä¸¦è¼¸å‡ºçµæœã€‚
#     """
#     if len(sys.argv) != 2:
#         print("Usage: python simplified_image_understanding_mps.py <path_to_blip3o_model>")
#         sys.exit(1)

#     model_path = os.path.expanduser(sys.argv[1])
    
#     # --- åˆå§‹åŒ–æ¨¡å‹ ---
#     print("Loading model and processor...")
#     disable_torch_init() # æ ¹æ“šåŸå§‹è…³æœ¬
#     try:
#         # è¼‰å…¥ BLIP3o æ¨¡å‹çµ„ä»¶ (tokenizer, multi_model)
#         tokenizer, multi_model, _ = load_pretrained_model(model_path)
        
#         # ***é—œéµä¿®æ”¹*** å°‡æ¨¡å‹ç§»å‹•åˆ° MPS è¨­å‚™
#         multi_model.to(DEVICE)
#         print(f"Model moved to device: {next(multi_model.parameters()).device}")
        
#         # è¼‰å…¥ Qwen-VL processor (processor æœ¬èº«é€šå¸¸ä¸åŒ…å«å¤§å‹æ¨¡å‹æ¬Šé‡)
#         processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct")
        
#         print("Model and processor loaded successfully.")
#     except Exception as e:
#         print(f"Error loading model or processor: {e}")
#         import traceback
#         traceback.print_exc()
#         sys.exit(1)

#     # --- ç²å–ç”¨æˆ¶è¼¸å…¥ ---
#     print("\n--- Image Understanding (MPS) ---")
#     prompt = input("Enter your prompt/question about the image: ")
    
#     image_path = input("Enter the path to your image file: ").strip()
#     if not os.path.exists(image_path):
#         print(f"Error: File not found at {image_path}")
#         sys.exit(1)

#     try:
#         # æ‰“é–‹åœ–ç‰‡
#         img = Image.open(image_path).convert('RGB') 
#         print(f"Image loaded: {image_path}")
#     except Exception as e:
#         print(f"Error opening image: {e}")
#         sys.exit(1)

#     # --- è™•ç†ä¸¦ç²å–çµæœ ---
#     print("\nProcessing image and generating response...")
#     try:
#         result_text = process_image(prompt, img, multi_model, processor)
#         print("\n--- Model Response ---")
#         print(result_text)
#     except Exception as e:
#         print(f"Error during processing or generation: {e}")
#         import traceback
#         traceback.print_exc()
#         sys.exit(1)

# if __name__ == "__main__":
#     main()

# image_understanding_gradio_mps.py
import sys
import os
from PIL import Image
import torch
import gradio as gr
from transformers import AutoProcessor
import gc

# === åœ¨æ¨¡çµ„å±¤ç´šå®£å‘Šå…¨å±€è®Šæ•¸ ===
model = None
processor = None

# --- æª¢æŸ¥ MPS å¯ç”¨æ€§ ---
if not torch.backends.mps.is_available():
    if not torch.backends.mps.is_built():
        error_msg = "MPS not available because the current PyTorch install was not built with MPS enabled."
    else:
        error_msg = "MPS not available because the current macOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine."
    
    print(error_msg)
    with gr.Blocks() as demo_error:
        gr.Markdown(f"## Error\n\n{error_msg}")
    demo_error.launch(share=True)
    sys.exit(1)
else:
    print("MPS is available. Using device: mps")

# --- è¨­å®šç›®æ¨™è¨­å‚™ ---
DEVICE = 'mps'

# --- åŒ¯å…¥è‡ªå®šç¾©æ¨¡çµ„ ---
try:
    from blip3o.model.builder import load_pretrained_model
    from blip3o.utils import disable_torch_init
    from qwen_vl_utils import process_vision_info
except ImportError as e:
    error_msg = f"Error importing custom modules: {e}\nPlease ensure 'blip3o' and 'qwen_vl_utils' are correctly installed or in the Python path."
    print(error_msg)
    with gr.Blocks() as demo_import_error:
        gr.Markdown(f"## Import Error\n\n{error_msg}")
    demo_import_error.launch(share=True)
    sys.exit(1)

# --- åœ–åƒç†è§£æ ¸å¿ƒå‡½æ•¸ ---
def process_image(prompt: str, img: Image.Image) -> str:
    """
    ä½¿ç”¨ Qwen-VL æ¨¡å‹è™•ç†åœ–ç‰‡å’Œæç¤ºæ–‡å­—ï¼Œé€²è¡Œåœ–åƒç†è§£ã€‚
    """
    global model, processor

    if model is None or processor is None:
        return "Error: Model or processor not loaded yet. Please check server initialization."

    if img is None:
        return "Error: No image uploaded."
    if not prompt.strip():
        return "Error: Please enter a prompt/question."

    try:
        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": img},
                {"type": "text", "text": prompt},
            ],
        }]

        text_prompt_for_qwen = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        image_inputs, video_inputs = process_vision_info(messages)

        inputs = processor(
            text=[text_prompt_for_qwen],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )

        inputs = inputs.to(DEVICE)
        print(f"[Processing] Input tensors moved to device: {inputs.input_ids.device}")

        print("[Processing] Generating response...")
        generated_ids = model.generate(**inputs, max_new_tokens=2048)
        print(f"[Processing] Generated IDs device: {generated_ids.device}")

        input_token_len = inputs.input_ids.shape[1]
        generated_ids_trimmed = generated_ids[:, input_token_len:]

        generated_ids_trimmed_cpu = generated_ids_trimmed.to('cpu')
        output_text = processor.batch_decode(
            generated_ids_trimmed_cpu, skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]

        return output_text

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"[Error] Processing failed: {e}")
        print(error_details)
        return f"Error during processing: {str(e)}\nDetails:\n{error_details}"
    finally:
        # ç„¡è«–æˆåŠŸæˆ–å¤±æ•—ï¼Œéƒ½å˜—è©¦é‡‹æ”¾è³‡æº
        gc.collect()
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()


# --- åˆå§‹åŒ–æ¨¡å‹ ---
def initialize_model():
    global model, processor

    if len(sys.argv) != 2:
        raise ValueError("Usage: python image_understanding_gradio_mps.py <path_to_blip3o_model>")

    model_path = os.path.expanduser(sys.argv[1])

    print("Loading model and processor...")
    disable_torch_init()  # ç¦ç”¨ä¸å¿…è¦çš„åˆå§‹åŒ–

    try:
        # è¼‰å…¥æ¨¡å‹ï¼ˆæ³¨æ„ï¼šç¢ºèª load_pretrained_model æ”¯æ´ device='mps'ï¼‰
        tokenizer, multi_model, _ = load_pretrained_model(model_path, device=DEVICE)

        # ç¢ºä¿æ¨¡å‹åœ¨ MPS ä¸Š
        multi_model = multi_model.to(DEVICE)

        # è¼‰å…¥ Qwen-VL processor
        processor_local = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct")

        # è³¦å€¼çµ¦å…¨å±€è®Šæ•¸
        model = multi_model
        processor = processor_local

        print("âœ… Model and processor loaded successfully.")
        return "Model loaded successfully!"

    except Exception as e:
        print(f"âŒ Failed to load model: {e}")
        raise


# --- Gradio ä»‹é¢ ---
with gr.Blocks(title="BLIP3-o Image Understanding (MPS)") as demo:
    gr.Markdown("## BLIP3-o Image Understanding (MPS)")
    
    with gr.Row():
        with gr.Column(scale=2):
            image_input = gr.Image(label="Upload Image", type="pil")
            prompt_input = gr.Textbox(
                label="Question/Prompt",
                placeholder="Describe the image you want...",
                lines=2
            )
            run_btn = gr.Button("Analyze Image")
            clear_btn = gr.Button("Clear")

            gr.Examples(
                examples=[
                    [None, "What is the main object in this image?"],
                    [None, "Describe the scene in this image."],
                    [None, "Are there any people in this image? If so, what are they doing?"],
                ],
                inputs=[image_input, prompt_input],
                outputs=None,
                cache_examples=False,
                label="Example Prompts"
            )

        with gr.Column(scale=3):
            output_text = gr.Textbox(label="Model Response", lines=10, interactive=False)

    # äº‹ä»¶ç¶å®š
    run_btn.click(fn=process_image, inputs=[prompt_input, image_input], outputs=output_text)
    prompt_input.submit(fn=process_image, inputs=[prompt_input, image_input], outputs=output_text)

    def clear_inputs():
        return None, "", ""

    clear_btn.click(fn=clear_inputs, inputs=[], outputs=[image_input, prompt_input, output_text])


# === ä¸»ç¨‹å¼å…¥å£ ===
if __name__ == "__main__":
    print("ğŸš€ Starting BLIP3-o Gradio App...")

    try:
        initialize_model()
    except Exception as e:
        print(f"âŒ Model initialization failed: {e}")
        sys.exit(1)

    if model is None or processor is None:
        print("âŒ Model or processor is still None after initialization.")
        sys.exit(1)

    print("âœ… Ready to launch Gradio interface.")
    demo.launch(server_name="0.0.0.0", server_port=7861)